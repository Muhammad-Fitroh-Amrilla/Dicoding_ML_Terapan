# -*- coding: utf-8 -*-
"""Project_Recommendation_System

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17rG8FU2fh5vpYm6C0iKGzg9yYEm6u0bP

<h1> <b>Sistem Rekomendasi Film<b> <h1>

# Import Library

Mengimport library yang dibutuhkan
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""# Data Loading

Mengunduh data dari sumber https://www.kaggle.com/datasets/bandikarthik/movie-recommendation-system kemudian diunggah melalui *google drive*. Mounted drive ke colab lalu unzip file dan terkahir memuat data ke dalam bentuk dataframe.

variabel yang ada pada dataset:

* links : merupakan daftar link film tersebut.
* movies : merupakan daftar film yang tersedia.
* ratings : merupakan daftar penilaian yang diberikan pengguna terhadap film.
* tags : merupakan daftar kata kunci dari film tersebut

"""

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/MyDrive/Dataset AI/Movie_Recommendation.zip"

links = pd.read_csv('/content/Movie_Recommendation/links.csv')
movies = pd.read_csv('/content/Movie_Recommendation/movies.csv')
ratings = pd.read_csv('/content/Movie_Recommendation/ratings.csv')
tags = pd.read_csv('/content/Movie_Recommendation/tags.csv')

"""# EDA - Analisis Univariate 
Analisis univariate merupakan proses untuk mengeksplorasi dan menjelaskan setiap variabel dalam kumpulan data secara terpisah.

## Variabel links
"""

links.info()

"""Dapat dilihat bahwa :

*   Terdapat 34208 data dalam links
*   Terdapat 2 buah kolom bertipe int64 yaitu movieId dan imdbId 
*   Terdapat 1 buah kolom bertipe float64 yaitu tmdbId 


"""

print('Jumlah data link film : ', len(links.movieId.unique()))

"""Dapat dilihat bahwa jumlah data link film berdasarkan movieId sebanyak 34208

## Variabel movies
"""

movies.info()

"""Dapat dilihat bahwa :

*   Terdapat 34208 data dalam links
*   Terdapat 1 buah kolom bertipe int64 yaitu movieId
*   Terdapat 2 buah kolom bertipe object yaitu title dan genres 


"""

print('Jumlah data movie : ', len(movies.movieId.unique()))
print('Banyak judul film: ', len(movies.title.unique()))
print('Banyak Genre: ', len(movies.genres.unique()))

"""## Variabel ratings"""

ratings.info()

"""Dapat dilihat bahwa :

*   Terdapat 22884377 data dalam links
*   Terdapat 3 buah kolom bertipe int64 yaitu userId, movieId dan timestamp
*   Terdapat 1 buah kolom bertipe float64 yaitu rating

Karena data terlalu banyak, maka data yang akan digunakan hanya 30000 data saja
"""

# mengambil data sebanyak 30000
ratings = ratings.iloc[:30000,:]

# cek bentuk data 
ratings.shape

print('Jumlah data ratings dari user : ', len(ratings.userId.unique()))
print('Jumlah data ratings dari movie : ', len(ratings.movieId.unique()))

ratings.describe()

"""Dapat kita lihat dari nilai max dan min bahwa nilai rating terbesar yaitu 5 dan nilai rating terkecil yaitu 0.5

## Variabel tags
"""

tags.info()

"""Dapat dilihat bahwa :

*   Terdapat 586994 data dalam links
*   Terdapat 3 buah kolom bertipe int64 yaitu userId, movieId dan timestamp
*   Terdapat 1 buah kolom bertipe object yaitu tag

Karena data terlalu banyak, maka data yang akan digunakan hanya 30000 data saja
"""

tags = tags.iloc[:30000,:]

tags.shape

print('Banyak tag: ', len(tags.tag.unique()))

"""# Content Based Filtering

## Data Preprocessing

### Menggabungkan Movie
"""

import numpy as np
 
# Menggabungkan seluruh movieId pada kategori movie
movie_all = np.concatenate((
    links.movieId.unique(),
    movies.movieId.unique(),
    ratings.movieId.unique(),
    tags.movieId.unique(),
))
 
# Mengurutkan data dan menghapus data yang sama
movie_all = np.sort(np.unique(movie_all))
 
print('Jumlah seluruh data movie berdasarkan movieID: ', len(movie_all))

"""### Menggabungkan User"""

# Menggabungkan seluruh userId
user_all = np.concatenate((
    ratings.userId.unique(),
    tags.userId.unique(),
   
))
 
# Menghapus data yang sama kemudian mengurutkannya
user_all = np.sort(np.unique(user_all)) 
 
print('Jumlah seluruh user: ', len(user_all))

"""### Menggabungkan seluruh data dengan fitur nama film"""

# Definisikan dataframe rating ke dalam variabel all_movie_rate
all_movie_rate = ratings
all_movie_rate

all_movie_name = pd.merge(all_movie_rate, movies[['movieId','title','genres']], on='movieId', how='left')
all_movie_name

# Menggabungkan dataframe genres dengan all_movie_name dan memasukkannya ke dalam variabel all_movie
all_movie = pd.merge(all_movie_name, tags[['movieId','tag']], on='movieId', how='left')
all_movie

"""## Data Preparation

### Menangani missing value

Melakukan pengecekan terlebih dahulu apakah didalam dataset terdapat missing value dengan kode berikut :
"""

# cek missing value
all_movie.isnull().sum()

"""Terdapat 6849 missing value terhadap fitur tag. Karena tidak bisa mengidentifikasi nama film yang tidak memiliki data ‘tag’ ini termasuk ke dalam kategori film mana kita, oleh karena itu akan di drop fitur tag menggunakan dropna"""

all_movie_clean = all_movie.dropna()
all_movie_clean

# cek ulang missing value
all_movie_clean.isnull().sum()

"""Missing value sudah tidak ada

### Mengurutkan Data

Mengurutkan data secara ascending
"""

# mengurutkan film berdasarkan movieId ke dalam variabel fix_movie
fix_movie = all_movie_clean.sort_values('movieId', ascending=True)
fix_movie

# cek jumlah fix movie
len(fix_movie.movieId.unique())

# Membuat variabel preparation yang berisi dataframe fix_movie kemudian mengurutkan berdasarkan movieId
preparation = fix_movie
preparation.sort_values('movieId', ascending=True)

"""### Menangani data duplikat

Menghapus data yang duplikat dengan fungsi drop_duplicates(). Dalam hal ini, membuang data duplikat pada kolom ‘movieId’.
"""

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('movieId')
preparation

"""### Konversi data menjadi list

Melakukan konversi data series menjadi list. Dalam hal ini, menggunakan fungsi tolist() dari library numpy.
"""

# Mengonversi data series ‘movieId’ menjadi dalam bentuk list
movie_id = preparation['movieId'].tolist()
 
# Mengonversi data series ‘title’ menjadi dalam bentuk list
movie_name = preparation['title'].tolist()
 
# Mengonversi data series ‘genres’ menjadi dalam bentuk list
movie_genre = preparation['genres'].tolist()
 
print(len(movie_id))
print(len(movie_name))
print(len(movie_genre))

"""### Membuat Dictionary
Membuat dictionary untuk menentukan pasangan key-value pada data movie_id, movie_name, dan movie_genre yang telah disiapkan sebelumnya
"""

# Membuat dictionary untuk data ‘movie_id’, ‘movie_name’, dan ‘movie_genre’
movie_new = pd.DataFrame({
    'id': movie_id,
    'movie_name': movie_name,
    'genre': movie_genre
})
movie_new

"""## Modelling

### TF-IDF Vectorizer
Digunakan untuk menemukan representasi fitur penting dari setiap kategori film.
Menggunakan fungsi tfidfvectorizer() dari library sklearn.
"""

from sklearn.feature_extraction.text import TfidfVectorizer
 
# Inisialisasi TfidfVectorizer
tfid = TfidfVectorizer()
 
# Melakukan perhitungan idf pada data genre
tfid.fit(movie_new['genre']) 
 
# Mapping array dari fitur index integer ke fitur nama
tfid.get_feature_names()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfid.fit_transform(movie_new['genre']) 
 
# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Dapat dilihat matriks yang dimiliki berukuran (559053, 21). Nilai 559053 merupakan ukuran data dan 21 merupakan matrik kategori film"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan jenis film
# Baris diisi dengan nama film

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tfid.get_feature_names(),
    index=movie_new.movie_name
).sample(10, axis=1,replace=True).sample(10, axis=0,replace=True)

"""### Cosine Similarity 
Menghitung derajat kesamaan (similarity degree) antar film dengan teknik cosine similarity. Dengan menggunakan fungsi cosine_similarity dari library sklearn. 
"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama film
cosine_sim_df = pd.DataFrame(cosine_sim, index=movie_new['movie_name'], columns=movie_new['movie_name'])
print('Shape:', cosine_sim_df.shape)
 
# Melihat similarity matrix pada setiap film 
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""## Evaluasi

### Mendapatkan Rekomendasi
Membuat fungsi movie_recommendations dengan beberapa parameter sebagai berikut:

* Nama_movie : Nama judul dari movie tersebut (index kemiripan dataframe).
* Similarity_data : Dataframe mengenai similarity yang telah kita didefinisikan sebelumnya
* Items : Nama dan fitur yang digunakan untuk mendefinisikan kemiripan, dalam hal ini adalah ‘movie_name’ dan ‘genre’.
* k : Banyak rekomendasi yang ingin diberikan.
"""

def movie_recommendations(nama_movie, similarity_data=cosine_sim_df, items=movie_new[['movie_name', 'genre']], k=5):
   
 
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan    
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_movie].to_numpy().argpartition(
        range(-1, -k, -1))
    
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # Drop nama_movie agar nama movie yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_movie, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

movie_new[movie_new.movie_name.eq('Toy Story (1995)')]

# mendapatkan rekomendasi film yang mirip dengan 'Toy Story (1995)' 
movie_recommendations('Toy Story (1995)')

"""# Collaborative Filtering

## Data Understanding
Supaya tidak tertukar dengan fitur ‘rating’ pada data, kita ubah nama variabel rating menjadi df
"""

# Membaca dataset
df = ratings
df

"""Dapat dilihat, data ratings memiliki 30000 baris dan 4 kolom

## Data Preparation

### Encode fitur userId dan movieId
Melakukan persiapan data untuk menjadikan (encode) fitur ‘userId’ dan ‘movieID’ ke dalam indeks integer
"""

# Mengubah userId menjadi list tanpa nilai yang sama
user_ids = df['userId'].unique().tolist()
print('list userId: ', user_ids)
 
# Melakukan encoding userId
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userId : ', user_to_user_encoded)
 
# Melakukan proses encoding angka ke ke userId
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userId: ', user_encoded_to_user)

# Mengubah movieId menjadi list tanpa nilai yang sama
movie_ids = df['movieId'].unique().tolist()
 
# Melakukan proses encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
 
# Melakukan proses encoding angka ke movieId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

"""### Memetakan userId dan movieId
Petakan userId dan movieId ke dataframe yang berkaitan.
"""

# Mapping userId ke dataframe genres
df['genres'] = df['userId'].map(user_to_user_encoded)
 
# Mapping movieD ke dataframe movies
df['movies'] = df['movieId'].map(movie_to_movie_encoded)

"""### Cek data dan ubah nilai rating
Terakhir, cek beberapa hal dalam data seperti jumlah user, jumlah movie, dan mengubah nilai rating menjadi float, cek nilai minimum dan maximum
"""

num_users = len(user_to_user_encoded)
print(num_users)
 
num_movie = len(movie_encoded_to_movie)
print(num_movie)
 
df['ratings'] = df['rating'].values.astype(np.float32)
 
min_rating = min(df['rating'])
 
max_rating = max(df['rating'])
 
print('Number of User: {}, Number of movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

"""### Membagi data untuk latih dan validasi

membagi data latih dan validasi dengan komposisi 80:20
"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data genres dan movies menjadi satu value
x = df[['genres', 'movies']].values
 
# Membuat variabel y untuk membuat ratings dari hasil 
y = df['ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""## Modelling

### Proses Latih

Membuat class RecommenderNet dengan keras Model class. Model ini menggunakan Binary Crossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer, dan root mean squared error (RMSE) sebagai metrics evaluation
"""

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movies
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movies bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2) 
 
    x = dot_user_movie + user_bias + movie_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""## Evaluasi

Evaluasi dilakukan dengan menggunakan matriks Root Mean Squared Error (RMSE). Root Mean Square Error adalah hasil dari akar kuadrat Mean Square Error. Mean Square Error yaitu menghitung jumlah selisih kuadrat rata-rata nilai sebenarnya dengan nilai prediksi.
"""

# Visualisasi RMSE
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Proses training model cukup smooth dan model konvergen pada epochs sekitar 50. Dari proses ini, kita memperoleh nilai error akhir sebesar sekitar 0.17 dan error pada data validasi sebesar 0.21 . Nilai tersebut cukup bagus untuk sistem rekomendasi.

### Mendapatkan Rekomendasi

Untuk mendapatkan rekomendasi film, pertama kita ambil sampel user secara acak dan definisikan variabel movie_not_watched yang merupakan daftar film yang belum pernah ditonton oleh pengguna
"""

movie_df = movie_new
df = pd.read_csv('/content/Movie_Recommendation/ratings.csv')
# mengambil data sebanyak 30000
df = df.iloc[:30000,:]

user_id = df.userId.sample(1).iloc[0]
movie_watched_by_user = df[df.userId == user_id]
 

movie_not_watched = movie_df[~movie_df['id'].isin(movie_watched_by_user.movieId.values)]['id'] 
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)
 
movie_not_watched = [[movie_to_movie_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)

# untuk memperoleh rekomendasi restoran, gunakan fungsi model.predict() dari library Keras
ratings = model.predict(user_movie_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_watched[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('movie with high ratings from user')
print('----' * 8)
 
top_movie_user = (
    movie_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)
 
movie_df_rows = movie_df[movie_df['id'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.movie_name, ':', row.genre)
 
print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)
 
recommended_movie = movie_df[movie_df['id'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.movie_name, ':', row.genre)